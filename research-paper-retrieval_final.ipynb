{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to Build a RAG-Powered Chatbot with Chat, Embed, and Rerank","metadata":{"id":"L-DQ_EAgbtMq"}},{"cell_type":"markdown","source":"*Read the accompanying [blog post here](https://txt.cohere.com/rag-chatbot).*","metadata":{"id":"yKSSJL3ZbtMx"}},{"cell_type":"markdown","source":"![Feature](https://github.com/cohere-ai/notebooks/blob/main/notebooks/images/rag-chatbot.png?raw=1)","metadata":{"id":"zmZhAeS8btMy"}},{"cell_type":"markdown","source":"In this notebook, you’ll learn how to build a chatbot that has RAG capabilities, enabling it to connect to external documents, ground its responses on these documents, and produce document citations in its responses.","metadata":{"id":"v7KLOeM0btMy"}},{"cell_type":"markdown","source":"Below is a diagram that provides an overview of what we’ll build, followed by a list of the key steps involved.\n\n![Overview](https://github.com/cohere-ai/notebooks/blob/main/notebooks/images/rag-chatbot-flow.png?raw=1)","metadata":{"id":"w40-8g8obtM0"}},{"cell_type":"markdown","source":"Setup phase:\n- Step 0: Ingest the documents – get documents, chunk, embed, and index.\n\nFor each user-chatbot interaction:\n- Step 1: Get the user message\n- Step 2: Call the Chat endpoint in query-generation mode\n- If at least one query is generated\n    - Step 3: Retrieve and rerank relevant documents\n    - Step 4: Call the Chat endpoint in document mode to generate a grounded response with citations\n- If no query is generated\n    - Step 4: Call the Chat endpoint in normal mode to generate a response\n\nThroughout the conversation:\n- Append the user-chatbot interaction to the conversation thread\n- Repeat with every interaction","metadata":{"id":"g7tGeStKbtM1"}},{"cell_type":"code","source":"! pip install openai tiktoken","metadata":{"id":"ytwx80NpmLa7","outputId":"6635be8a-5b6f-4692-eeda-5433d1f0a85b","execution":{"iopub.status.busy":"2023-11-26T20:54:42.151919Z","iopub.execute_input":"2023-11-26T20:54:42.152257Z","iopub.status.idle":"2023-11-26T20:54:57.186544Z","shell.execute_reply.started":"2023-11-26T20:54:42.152230Z","shell.execute_reply":"2023-11-26T20:54:57.185598Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting openai\n  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/30/1d/27c3571504fb6fb1e9f7c906d93590ead22f5f34910489e155ee28512eeb/openai-1.3.5-py3-none-any.whl.metadata\n  Downloading openai-1.3.5-py3-none-any.whl.metadata (16 kB)\nCollecting tiktoken\n  Obtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/f4/2e/0adf6e264b996e263b1c57cad6560ffd5492a69beb9fd779ed0463d486bc/tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: anyio<4,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (3.7.1)\nCollecting distro<2,>=1.7.0 (from openai)\n  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/a2/65/6940eeb21dcb2953778a6895281c179efd9100463ff08cb6232bb6480da7/httpx-0.25.2-py3-none-any.whl.metadata\n  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.10.12)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.5 in /opt/conda/lib/python3.10/site-packages (from openai) (4.5.0)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\nDownloading openai-1.3.5-py3-none-any.whl (220 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.8/220.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx-0.25.2-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: httpcore, distro, tiktoken, httpx, openai\nSuccessfully installed distro-1.8.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.5 tiktoken-0.5.1\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install PyPDF2","metadata":{"execution":{"iopub.status.busy":"2023-11-26T20:54:57.188729Z","iopub.execute_input":"2023-11-26T20:54:57.189051Z","iopub.status.idle":"2023-11-26T20:55:09.987763Z","shell.execute_reply.started":"2023-11-26T20:54:57.189020Z","shell.execute_reply":"2023-11-26T20:55:09.986694Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install cohere hnswlib unstructured -q","metadata":{"execution":{"iopub.status.busy":"2023-11-26T20:55:09.989105Z","iopub.execute_input":"2023-11-26T20:55:09.989396Z","iopub.status.idle":"2023-11-26T20:56:01.097438Z","shell.execute_reply.started":"2023-11-26T20:55:09.989359Z","shell.execute_reply":"2023-11-26T20:56:01.096295Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nimport cohere\nimport os\nimport hnswlib\nimport json\nimport uuid\nfrom typing import List, Dict\nfrom unstructured.partition.html import partition_html\n# from unstructured.chunking.title import chunk_by_title\nfrom PyPDF2 import PdfReader\nimport requests\nfrom io import BytesIO\nimport PyPDF2\n\nos.environ[\"COHERE_API_KEY\"]= \"vOXc8PMABEh4ZgaSmxiirTsGom3Ttq482wdMmYBC\"\n\nco = cohere.Client(os.environ[\"COHERE_API_KEY\"])","metadata":{"id":"OXVW5mrNbtNA","execution":{"iopub.status.busy":"2023-11-26T20:56:01.098932Z","iopub.execute_input":"2023-11-26T20:56:01.099302Z","iopub.status.idle":"2023-11-26T20:56:04.264023Z","shell.execute_reply.started":"2023-11-26T20:56:01.099265Z","shell.execute_reply":"2023-11-26T20:56:04.262938Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Enable text wrapping in Google colab\n\nfrom IPython.display import HTML, display\n\ndef set_css():\n  display(HTML('''\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  '''))\nget_ipython().events.register('pre_run_cell', set_css)","metadata":{"id":"7qZxrdS3btNB","execution":{"iopub.status.busy":"2023-11-26T20:56:04.266672Z","iopub.execute_input":"2023-11-26T20:56:04.267068Z","iopub.status.idle":"2023-11-26T20:56:04.272620Z","shell.execute_reply.started":"2023-11-26T20:56:04.267041Z","shell.execute_reply":"2023-11-26T20:56:04.271757Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Documents component","metadata":{"id":"cQjdqYLcbtNC"}},{"cell_type":"code","source":"class Documents:\n    \"\"\"\n    A class representing a collection of documents.\n\n    Parameters:\n    sources (list): A list of dictionaries representing the sources of the documents. Each dictionary should have 'title' and 'url' keys.\n\n    Attributes:\n    sources (list): A list of dictionaries representing the sources of the documents.\n    docs (list): A list of dictionaries representing the documents, with 'title', 'content', and 'url' keys.\n    docs_embs (list): A list of the associated embeddings for the documents.\n    retrieve_top_k (int): The number of documents to retrieve during search.\n    rerank_top_k (int): The number of documents to rerank after retrieval.\n    docs_len (int): The number of documents in the collection.\n    index (hnswlib.Index): The index used for document retrieval.\n\n    Methods:\n    load(): Loads the data from the sources and partitions the HTML content into chunks.\n    embed(): Embeds the documents using the Cohere API.\n    index(): Indexes the documents for efficient retrieval.\n    retrieve(query): Retrieves documents based on the given query.\n\n    \"\"\"\n\n    def __init__(self, sources: List[Dict[str, str]]):\n        self.sources = sources\n        self.docs = []\n        self.docs_embs = []\n        self.retrieve_top_k = 10\n        self.rerank_top_k = 3\n        self.load()\n        self.embed()\n        self.index()\n\n    def is_title_line(self,line):\n    # A simple heuristic to identify title lines. You might need to adjust this.\n      return line.isupper() and len(line) > 10\n\n    def chunk_by_title(self, text, max_tokens=300):\n      chunks = []\n      current_chunk = []\n      current_token_count = 0\n\n      for line in text.split('\\n'):\n          line_tokens = line.split()\n          line_token_count = len(line_tokens)\n\n          # Check if adding this line would exceed the token limit\n          if current_token_count + line_token_count > max_tokens:\n              chunks.append('\\n'.join(current_chunk))\n              current_chunk = []\n              current_token_count = 0\n\n          if self.is_title_line(line):\n              if current_chunk:\n                  chunks.append('\\n'.join(current_chunk))\n                  current_chunk = []\n                  current_token_count = 0\n\n          current_chunk.append(line)\n          current_token_count += line_token_count\n\n      if current_chunk:\n          chunks.append('\\n'.join(current_chunk))\n\n      return chunks\n\n\n    def load(self) -> None:\n        \"\"\"\n        Loads the documents from the sources and chunks the HTML content.\n        \"\"\"\n        print(\"Loading documents...\")\n\n        for source in self.sources:\n          response = requests.get(source['url'])\n          if response.status_code != 200:\n            print('Failed to retrieve the PDF')\n            return []\n\n            # Explicitly decode the content as UTF-8 if needed\n\n          pdf_content = BytesIO(response.content)\n          reader = PyPDF2.PdfReader(pdf_content)\n          all_chunks = []\n          for page_num in range(len(reader.pages)):\n            page = reader.pages[page_num]\n            text = page.extract_text()\n            if text:\n              if isinstance(text, bytes):\n                text = text.decode('utf-8')\n              chunks = self.chunk_by_title(text)\n              for chunk in chunks:\n                # print(str(chunk))\n                # print('-'*100)\n                # print(len(str(chunk).split()))\n                self.docs.append(\n                    {\n                        \"title\": source[\"title\"],\n                        \"text\": str(chunk),\n                        \"url\": source[\"url\"],\n                    }\n                )\n        # for source in self.sources:\n        #     response = requests.get(source[\"url\"])\n        #     if response.status_code == 200:\n        #       pdf_content = BytesIO(response.content)\n        #     else:\n        #       print('Failed to retrieve the PDF')\n        #       pdf_content = None\n        #     if pdf_content:\n        #       reader = PyPDF2.PdfReader(pdf_content)\n        #       for page_num in range(len(reader.pages)):\n        #         page = reader.pages[page_num]\n        #         chunks = chunk_by_title(page.extract_text())\n        #         for chunk in chunks:\n        #           self.docs.append(\n        #               {\n        #                   \"title\": source[\"title\"],\n        #                   \"text\": str(chunk),\n        #                   \"url\": source[\"url\"],\n        #               }\n        #           )\n            #reader = PdfReader(source['url'])\n\n    def embed(self) -> None:\n        \"\"\"\n        Embeds the documents using the Cohere API.\n        \"\"\"\n        print(\"Embedding documents...\")\n\n        batch_size = 90\n        self.docs_len = len(self.docs)\n\n        for i in range(0, self.docs_len, batch_size):\n            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n            texts = [item[\"text\"] for item in batch]\n            docs_embs_batch = co.embed(\n                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n            ).embeddings\n            self.docs_embs.extend(docs_embs_batch)\n\n    def index(self) -> None:\n        \"\"\"\n        Indexes the documents for efficient retrieval.\n        \"\"\"\n        print(\"Indexing documents...\")\n\n        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n\n        print(f\"Indexing complete with {self.idx.get_current_count()} documents.\")\n\n    def retrieve(self, query: str) -> List[Dict[str, str]]:\n        \"\"\"\n        Retrieves documents based on the given query.\n\n        Parameters:\n        query (str): The query to retrieve documents for.\n\n        Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the retrieved documents, with 'title', 'text', and 'url' keys.\n        \"\"\"\n        docs_retrieved = []\n        query_emb = co.embed(\n            texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n        ).embeddings\n\n        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n\n        docs_to_rerank = []\n        for doc_id in doc_ids:\n            docs_to_rerank.append(self.docs[doc_id][\"text\"])\n\n        rerank_results = co.rerank(\n            query=query,\n            documents=docs_to_rerank,\n            top_n=self.rerank_top_k,\n            model=\"rerank-english-v2.0\",\n        )\n\n        doc_ids_reranked = []\n        for result in rerank_results:\n            doc_ids_reranked.append(doc_ids[result.index])\n\n        for doc_id in doc_ids_reranked:\n            docs_retrieved.append(\n                {\n                    \"title\": self.docs[doc_id][\"title\"],\n                    \"text\": self.docs[doc_id][\"text\"],\n                    \"url\": self.docs[doc_id][\"url\"],\n                }\n            )\n\n        return docs_retrieved","metadata":{"id":"WkuYHKXXbtNC","outputId":"326f919a-de97-4052-fee0-b0dd1e75d619","execution":{"iopub.status.busy":"2023-11-26T20:56:04.273738Z","iopub.execute_input":"2023-11-26T20:56:04.273974Z","iopub.status.idle":"2023-11-26T20:56:04.303126Z","shell.execute_reply.started":"2023-11-26T20:56:04.273953Z","shell.execute_reply":"2023-11-26T20:56:04.302173Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}}]},{"cell_type":"markdown","source":"### Chatbot component","metadata":{"id":"MKcnmZnxbtNF"}},{"cell_type":"code","source":"class Chatbot:\n    \"\"\"\n    A class representing a chatbot.\n\n    Parameters:\n    docs (Documents): An instance of the Documents class representing the collection of documents.\n\n    Attributes:\n    conversation_id (str): The unique ID for the conversation.\n    docs (Documents): An instance of the Documents class representing the collection of documents.\n\n    Methods:\n    generate_response(message): Generates a response to the user's message.\n    retrieve_docs(response): Retrieves documents based on the search queries in the response.\n\n    \"\"\"\n\n    def __init__(self, docs: Documents):\n        self.docs = docs\n        self.conversation_id = str(uuid.uuid4())\n\n    def generate_response(self, message: str):\n        \"\"\"\n        Generates a response to the user's message.\n\n        Parameters:\n        message (str): The user's message.\n\n        Yields:\n        Event: A response event generated by the chatbot.\n\n        Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the retrieved documents.\n\n        \"\"\"\n        # Generate search queries (if any)\n        response = co.chat(message=message, search_queries_only=True)\n\n        # If there are search queries, retrieve documents and respond\n        if response.search_queries:\n            print(\"Retrieving information...\")\n\n            documents = self.retrieve_docs(response)\n\n            response = co.chat(\n                message=message,\n                documents=documents,\n                conversation_id=self.conversation_id,\n                stream=True,\n            )\n            for event in response:\n                yield event\n\n        # If there is no search query, directly respond\n        else:\n            response = co.chat(\n                message=message,\n                conversation_id=self.conversation_id,\n                stream=True\n            )\n            for event in response:\n                yield event\n\n    def retrieve_docs(self, response) -> List[Dict[str, str]]:\n        \"\"\"\n        Retrieves documents based on the search queries in the response.\n\n        Parameters:\n        response: The response object containing search queries.\n\n        Returns:\n        List[Dict[str, str]]: A list of dictionaries representing the retrieved documents.\n\n        \"\"\"\n        # Get the query(s)\n        queries = []\n        for search_query in response.search_queries:\n            queries.append(search_query[\"text\"])\n\n        # Retrieve documents for each query\n        retrieved_docs = []\n        for query in queries:\n            retrieved_docs.extend(self.docs.retrieve(query))\n\n        # # Uncomment this code block to display the chatbot's retrieved documents\n        # print(\"DOCUMENTS RETRIEVED:\")\n        # for idx, doc in enumerate(retrieved_docs):\n        #     print(f\"doc_{idx}: {doc}\")\n        # print(\"\\n\")\n\n        return retrieved_docs","metadata":{"id":"xmmiYR87btNF","outputId":"28e4a9ee-0acd-480c-b672-13bef4c7d871","execution":{"iopub.status.busy":"2023-11-26T20:56:04.304604Z","iopub.execute_input":"2023-11-26T20:56:04.304881Z","iopub.status.idle":"2023-11-26T20:56:04.317948Z","shell.execute_reply.started":"2023-11-26T20:56:04.304858Z","shell.execute_reply":"2023-11-26T20:56:04.317136Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}}]},{"cell_type":"markdown","source":"### App component","metadata":{"id":"isFJFwFLbtNG"}},{"cell_type":"code","source":"class App:\n    def __init__(self, chatbot: Chatbot):\n        \"\"\"\n        Initializes an instance of the App class.\n\n        Parameters:\n        chatbot (Chatbot): An instance of the Chatbot class.\n\n        \"\"\"\n        self.chatbot = chatbot\n\n    def run(self):\n        \"\"\"\n        Runs the chatbot application.\n\n        \"\"\"\n        while True:\n            # Get the user message\n            message = input(\"User: \")\n\n            # Typing \"quit\" ends the conversation\n            if message.lower() == \"quit\":\n                print(\"Ending chat.\")\n                break\n            else:\n                print(f\"User: {message}\")\n\n            # Get the chatbot response\n            response = self.chatbot.generate_response(message)\n\n            # Print the chatbot response\n            print(\"Chatbot:\")\n            flag = False\n            for event in response:\n                # Text\n                if event.event_type == \"text-generation\":\n                    print(event.text, end=\"\")\n\n                # Citations\n                if event.event_type == \"citation-generation\":\n                    if not flag:\n                        print(\"\\n\\nCITATIONS:\")\n                        flag = True\n                    print(event.citations)\n\n            print(f\"\\n{'-'*100}\\n\")","metadata":{"id":"4vcOWzpfbtNG","outputId":"c317162e-c054-4055-bd40-ac48cd631978","execution":{"iopub.status.busy":"2023-11-26T20:56:04.319116Z","iopub.execute_input":"2023-11-26T20:56:04.319747Z","iopub.status.idle":"2023-11-26T20:56:04.332707Z","shell.execute_reply.started":"2023-11-26T20:56:04.319721Z","shell.execute_reply":"2023-11-26T20:56:04.331624Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}}]},{"cell_type":"markdown","source":"### Define the documents","metadata":{"id":"Eo9G7wacbtNH"}},{"cell_type":"code","source":"# Define the sources for the documents\n# As an example, we'll use LLM University's Module 1: What are Large Language Models?\n# https://docs.cohere.com/docs/intro-large-language-models\n\nsources = [\n    {\n        \"title\": \"Text Line Segmentation of Historical Documents: a Survey\",\n        \"url\": \"https://arxiv.org/pdf/0704.1267.pdf\"\n    },\n    {\n        \"title\": \"Riemannian level-set methods for tensor-valued data\",\n        \"url\": \"https://arxiv.org/pdf/0705.0214.pdf\"\n    },\n    {\n        \"title\": \"Multiresolution Approximation of Polygonal Curves in Linear Complexity\",\n        \"url\": \"https://arxiv.org/pdf/0705.0449.pdf\"\n    },\n    {\n        \"title\": \"Medical Image Segmentation and Localization using Deformable Templates\",\n        \"url\": \"https://arxiv.org/pdf/0705.0781.pdf\"\n    },\n    {\n        \"title\": \"Enhancement of Noisy Planar Nuclear Medicine Images using Mean Field\\n  Annealing\",\n        \"url\": \"https://arxiv.org/pdf/0705.0828.pdf\"\n    },\n    {\n        \"title\": \"An Independent Evaluation of Subspace Face Recognition Algorithms\",\n        \"url\": \"https://arxiv.org/pdf/0705.0952.pdf\"\n    },\n    {\n        \"title\": \"MI image registration using prior knowledge\",\n        \"url\": \"https://arxiv.org/pdf/0705.3593.pdf\"\n    },\n    {\n        \"title\": \"Automatic Detection of Pulmonary Embolism using Computational\\n  Intelligence\",\n        \"url\": \"https://arxiv.org/pdf/0706.0300.pdf\"\n    },\n    {\n        \"title\": \"Variational local structure estimation for image super-resolution\",\n        \"url\": \"https://arxiv.org/pdf/0709.1771.pdf\"\n    },\n    {\n        \"title\": \"Bandwidth selection for kernel estimation in mixed multi-dimensional\\n  spaces\",\n        \"url\": \"https://arxiv.org/pdf/0709.1920.pdf\"\n    },\n    {\n        \"title\": \"Supervised learning on graphs of spatio-temporal similarity in satellite\\n  image sequences\",\n        \"url\": \"https://arxiv.org/pdf/0709.3013.pdf\"\n    },\n    {\n        \"title\": \"Graph rigidity, Cyclic Belief Propagation and Point Pattern Matching\",\n        \"url\": \"https://arxiv.org/pdf/0710.0043.pdf\"\n    },\n    {\n        \"title\": \"High-Order Nonparametric Belief-Propagation for Fast Image Inpainting\",\n        \"url\": \"https://arxiv.org/pdf/0710.0243.pdf\"\n    },\n    {\n        \"title\": \"An Affinity Propagation Based method for Vector Quantization Codebook\\n  Design\",\n        \"url\": \"https://arxiv.org/pdf/0710.2037.pdf\"\n    },\n    {\n        \"title\": \"Comparison and Combination of State-of-the-art Techniques for\\n  Handwritten Character Recognition: Topping the MNIST Benchmark\",\n        \"url\": \"https://arxiv.org/pdf/0710.2231.pdf\"\n    }\n]\n","metadata":{"id":"abxqs4_zbtNI","outputId":"e05058df-99ed-422c-a255-339b93167ef7","execution":{"iopub.status.busy":"2023-11-26T20:56:04.334214Z","iopub.execute_input":"2023-11-26T20:56:04.334537Z","iopub.status.idle":"2023-11-26T20:56:04.347980Z","shell.execute_reply.started":"2023-11-26T20:56:04.334511Z","shell.execute_reply":"2023-11-26T20:56:04.347117Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}}]},{"cell_type":"markdown","source":"### Process the documents","metadata":{"id":"xvB-bOj9btNI"}},{"cell_type":"code","source":"# Create an instance of the Documents class with the given sources\ndocuments = Documents(sources)","metadata":{"id":"hJvQ4VPJbtNI","outputId":"251ce232-c2bc-40db-fcd3-1d7795564a04","execution":{"iopub.status.busy":"2023-11-26T20:56:04.349467Z","iopub.execute_input":"2023-11-26T20:56:04.349840Z","iopub.status.idle":"2023-11-26T20:56:33.753244Z","shell.execute_reply.started":"2023-11-26T20:56:04.349807Z","shell.execute_reply":"2023-11-26T20:56:33.752312Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}},{"name":"stdout","text":"Loading documents...\nEmbedding documents...\nIndexing documents...\nIndexing complete with 391 documents.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Run the chatbot","metadata":{"id":"20YPSu1RbtNJ"}},{"cell_type":"code","source":"# Create an instance of the Chatbot class with the Documents instance\nchatbot = Chatbot(documents)\n\n# Create an instance of the App class with the Chatbot instance\napp = App(chatbot)\n\n# Run the chatbot\napp.run()","metadata":{"id":"ilbOV6qCbtNJ","outputId":"8c165cd8-32e3-4dd7-b042-08fef3a9e7bb","execution":{"iopub.status.busy":"2023-11-26T20:56:33.754609Z","iopub.execute_input":"2023-11-26T20:56:33.754920Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n  <style>\n    pre {\n        white-space: pre-wrap;\n    }\n  </style>\n  "},"metadata":{}}]}]}